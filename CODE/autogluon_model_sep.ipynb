{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2030,"status":"ok","timestamp":1700419949356,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"NS-Gavs8XHm3","outputId":"450c8ca4-85cb-4d96-a34a-ce49aca561e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaepJ3RmXTFo"},"outputs":[],"source":["!pip install autogluon\n","!pip install mxnet~=1.9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwDLmRBhe4Zj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","import holidays\n","from autogluon.tabular import TabularDataset, TabularPredictor\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5mLOMhPXNGE"},"outputs":[],"source":["path = '/content/drive/MyDrive/DACON/jeju_price/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIn-ird9e4Zl"},"outputs":[],"source":["train = pd.read_csv(path + 'train.csv')\n","international_trade = pd.read_csv(path + 'international_trade.csv')\n","test = pd.read_csv(path + 'test.csv')\n","\n","train['item_id'] = train.ID.str[0:6]\n","train = train[(train[\"item_id\"] != \"BC_B_S\") & (train[\"item_id\"] != \"BC_C_S\") & (train[\"item_id\"] != \"CB_A_S\") & (train[\"item_id\"] != \"CR_D_S\") & (train[\"item_id\"] != \"CR_E_S\") & (train[\"item_id\"] != \"RD_C_S\")]\n","train = train.reset_index(drop = True)\n","train.drop(columns=['item_id'], inplace=True)\n","\n","test['item_id'] = test.ID.str[0:6]\n","test = test[(test[\"item_id\"] != \"BC_B_S\") & (test[\"item_id\"] != \"BC_C_S\") & (test[\"item_id\"] != \"CB_A_S\") & (test[\"item_id\"] != \"CR_D_S\") & (test[\"item_id\"] != \"CR_E_S\") & (test[\"item_id\"] != \"RD_C_S\")]\n","test = test.reset_index(drop = True)\n","test.drop(columns=['item_id'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW0WvVk4e4Zm"},"outputs":[],"source":["def group_season(df):\n","    df.loc[(df['month'] == 3) | (df['month'] == 4) | (df['month'] == 5), 'season'] = '봄'\n","    df.loc[(df['month'] == 6) | (df['month'] == 7) | (df['month'] == 8), 'season'] = '여름'\n","    df.loc[(df['month'] == 9) | (df['month'] == 10) | (df['month'] == 11), 'season'] = '가을'\n","    df.loc[(df['month'] == 12) | (df['month'] == 1) | (df['month'] == 2), 'season'] = '겨울'\n","    return df['season']\n","\n","def holiday(df):\n","    kr_holidays = holidays.KR()\n","    df['holiday'] = df.timestamp.apply(lambda x: 'holiday' if x in kr_holidays else 'non-holiday')\n","    return df['holiday']\n","\n","def cyclical_feature(df, time=12):\n","    df['sin_time'] = np.sin(2*np.pi*df.month/time)\n","    df['cos_time'] = np.cos(2*np.pi*df.month/time)\n","\n","def post_preprocessing(test, submission):\n","    idx_list = test[(test['Weekday'] == 6)].index\n","    submission.loc[idx_list, 'answer'] = 0 # Weekday == 6 (일요일)이면 가격 0원\n","    submission['answer'] = submission['answer'].apply(lambda x: max(0, x)) # 가격에 음수가 있다면 가격 0원으로 변경\n","    return submission\n","\n","def determine_harvest_weight(item, month):\n","    harvest_times = {\n","    'TG': {'main': [(10, 1)]},  # 감귤: 10월부터 이듬해 1월까지\n","    'BC': {'main': [(4, 6), (9, 11)]},  # 브로콜리: 4월-6월, 9월-11월\n","    'RD': {'main': [(5, 6), (11, 12)]},  # 무: 5월, 11월\n","    'CR': {'main': [(7, 8), (10, 11)]},  # 당근: 7월-8월, 10월-12월\n","    'CB': {'main': [(6, 6), (11, 11)]}  # 양배추: 6월, 11월\n","}\n","    main_harvest = harvest_times[item]['main']\n","    for start, end in main_harvest:\n","        if start <= month <= end:\n","            return 1\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDluuBJRe4Zm"},"outputs":[],"source":["class DataPreprocessing:\n","    def __init__(self, train, test):\n","        self.train = train\n","        self.test = test\n","\n","    @staticmethod\n","    def label_encode(train, test):\n","        categorical_col = ['item', 'corporation', 'location', 'season', 'holiday', 'total_item_value',\n","                           'item_month_Weekday', 'item_corp_Weekday', 'item_location_Weekday', 'item_year_season', 'item_weight']\n","\n","        for i in categorical_col:\n","            le = LabelEncoder()\n","            train[i] = le.fit_transform(train[i])\n","            test[i] = le.transform(test[i])\n","\n","        return train, test\n","\n","    @staticmethod\n","    def remove_outliers(train):\n","        print('Remove outliers')\n","        train.loc[(train['Weekday'] == 6) & (train['price(원/kg)'] >= 0), 'price(원/kg)'] = 0\n","        return train\n","\n","    @staticmethod\n","    def preprocessing(data):\n","        print('Preprocessing Start')\n","        # time feature\n","        data['year'] = data['timestamp'].apply(lambda x: int(x[0:4]))\n","        data['month'] = data['timestamp'].apply(lambda x: int(x[5:7]))\n","        data['Weekday'] = pd.to_datetime(data['timestamp']).dt.weekday\n","        data['is_weekend'] = data['Weekday'].apply(lambda x: 1 if x >= 6 else 0)\n","        data['year'] = data['year'] - 2019\n","        data['season'] = group_season(data)\n","        data['holiday'] = holiday(data)\n","        cyclical_feature(data)\n","\n","        # item feature\n","        data['total_item_value'] = data['item']+data['corporation']+data['location']\n","        data['item_month_Weekday'] = data['item'].astype(str) + \"_\" + data['month'].astype(str) + data['Weekday'].astype(str)\n","        data['item_corp_Weekday'] = data['item'].astype(str) + \"_\" + data['corporation'].astype(str) + data['Weekday'].astype(str)\n","        data['item_location_Weekday'] = data['item'].astype(str) + \"_\" + data['location'].astype(str) + data['Weekday'].astype(str)\n","        data['item_year_season'] = data['item'].astype(str) + \"_\" + data['year'].astype(str) + \"_\" + data['season'].astype(str)\n","\n","\n","        data['timestamp'] = pd.to_datetime(data['timestamp'])\n","        data['harvest_weight'] = data.apply(lambda row: determine_harvest_weight(row['item'], row['timestamp'].month), axis=1)\n","        # data['timestamp'] = data['timestamp'].view('int64') * 1e9\n","\n","        data['item_weight'] = data['item'].astype(str) + \"_\" + data['harvest_weight'].astype(str)\n","        return data\n","\n","    def fit(self):\n","        self.train = self.preprocessing(self.train)\n","        self.test = self.preprocessing(self.test)\n","\n","        self.train = self.remove_outliers(self.train)\n","\n","        x_train = self.train.drop(columns=['ID', 'supply(kg)', 'price(원/kg)'])\n","        y_train = self.train['price(원/kg)']\n","        x_test = self.test.drop(columns=['ID'])\n","\n","        # x_train, x_test = self.label_encode(x_train, x_test)\n","\n","        return x_train, y_train, x_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4128,"status":"ok","timestamp":1700419975571,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"6GbGvo9Fa45m","outputId":"d89e7f48-2ed9-4775-eb31-9c9dc216eacb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessing Start\n","Preprocessing Start\n","Remove outliers\n"]}],"source":["preprocessing = DataPreprocessing(train, test)\n","x, y, test = preprocessing.fit()\n","train_set = pd.concat([x, y], axis=1)\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1103)\n","train_autogluon = pd.concat([x, y], axis=1)\n","\n","\n","train_autogluon['week_of_year'] = train_autogluon['timestamp'].dt.isocalendar().week\n","test['week_of_year'] = test['timestamp'].dt.isocalendar().week\n","\n","train_autogluon['week_item'] = train_autogluon['week_of_year'].astype(str) + '_' + train_autogluon['item'].astype(str)\n","test['week_item'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str)\n","filtered_data = train_autogluon[train_autogluon['price(원/kg)'] > 0]\n","mean_encoded = filtered_data.groupby('week_item')['price(원/kg)'].mean().reset_index()\n","mean_encoded.rename(columns={'price(원/kg)': 'mean_encoded_price'}, inplace=True)\n","train_autogluon = pd.merge(train_autogluon, mean_encoded, on='week_item', how='left')\n","test = pd.merge(test, mean_encoded, on='week_item', how='left')\n","\n","train_autogluon['week_item_location'] = train_autogluon['week_of_year'].astype(str) + '_' + train_autogluon['item'].astype(str) + '_' + train_autogluon['location'].astype(str)\n","test['week_item_location'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['location'].astype(str)\n","filtered_data = train_autogluon[train_autogluon['price(원/kg)'] > 0]\n","mean_encoded_mwic = filtered_data.groupby('week_item_location')['price(원/kg)'].mean().reset_index()\n","mean_encoded_mwic.rename(columns={'price(원/kg)': 'mean_encoded_price_mwil'}, inplace=True)\n","train_autogluon = pd.merge(train_autogluon, mean_encoded_mwic, on='week_item_location', how='left')\n","test = pd.merge(test, mean_encoded_mwic, on='week_item_location', how='left')\n","\n","train_autogluon['week_item_corporation'] = train_autogluon['week_of_year'].astype(str) + '_' + train_autogluon['item'].astype(str) + '_' + train_autogluon['corporation'].astype(str)\n","test['week_item_corporation'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['corporation'].astype(str)\n","filtered_data_mwic = train_autogluon[train_autogluon['price(원/kg)'] > 0]\n","mean_encoded_mwic = filtered_data_mwic.groupby('week_item_corporation')['price(원/kg)'].mean().reset_index()\n","mean_encoded_mwic.rename(columns={'price(원/kg)': 'mean_encoded_price_mwic'}, inplace=True)\n","train_autogluon = pd.merge(train_autogluon, mean_encoded_mwic, on='week_item_corporation', how='left')\n","test = pd.merge(test, mean_encoded_mwic, on='week_item_corporation', how='left')\n","\n","train_autogluon['week_item_corporation_location'] = train_autogluon['week_of_year'].astype(str) + '_' + train_autogluon['item'].astype(str) + '_' + train_autogluon['corporation'].astype(str) + '_' + train_autogluon['location'].astype(str)\n","test['week_item_corporation_location'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['corporation'].astype(str) + '_' + test['location'].astype(str)\n","filtered_data_mwicl = train_autogluon[train_autogluon['price(원/kg)'] > 0]\n","mean_encoded_mwicl = filtered_data_mwicl.groupby('week_item_corporation_location')['price(원/kg)'].mean().reset_index()\n","mean_encoded_mwicl.rename(columns={'price(원/kg)': 'mean_encoded_price_mwicl'}, inplace=True)\n","train_autogluon = pd.merge(train_autogluon, mean_encoded_mwicl, on='week_item_corporation_location', how='left')\n","test = pd.merge(test, mean_encoded_mwicl, on='week_item_corporation_location', how='left')\n","\n","\n","\n","# train_autogluon['original_index'] = train_autogluon.index\n","# test['original_index'] = test.index\n","\n","# train_autogluon['item_id'] = train_autogluon['item'].astype(str) + '_' + train_autogluon['corporation'].astype(str) + '_' + train_autogluon['location'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['corporation'].astype(str) + '_' + test['location'].astype(str)\n","# weekly_avg = train_autogluon.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwicl'].mean().reset_index()\n","# weekly_avg['weekly_difference'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwicl'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference']]\n","# train_autogluon = pd.merge(train_autogluon, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train_autogluon = train_autogluon.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","\n","# train_autogluon['item_id'] = train_autogluon['item'].astype(str) + '_' + train_autogluon['corporation'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['corporation'].astype(str)\n","# weekly_avg = train_autogluon.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwic'].mean().reset_index()\n","# weekly_avg['weekly_difference_2'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwic'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_2']]\n","# train_autogluon = pd.merge(train_autogluon, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train_autogluon = train_autogluon.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","# train_autogluon['item_id'] = train_autogluon['item'].astype(str) + '_' + train_autogluon['location'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['location'].astype(str)\n","# weekly_avg = train_autogluon.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwil'].mean().reset_index()\n","# weekly_avg['weekly_difference_3'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwil'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_3']]\n","# train_autogluon = pd.merge(train_autogluon, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train_autogluon = train_autogluon.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","# train_autogluon['item_id'] = train_autogluon['item'].astype(str)\n","# test['item_id'] = test['item'].astype(str)\n","# weekly_avg = train_autogluon.groupby(['item_id', 'week_of_year'])['mean_encoded_price'].mean().reset_index()\n","# weekly_avg['weekly_difference_4'] = weekly_avg.groupby('item_id')['mean_encoded_price'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_4']]\n","# train_autogluon = pd.merge(train_autogluon, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train_autogluon = train_autogluon.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","train_autogluon['timestamp'] = train_autogluon['timestamp'].view('int64') * 1e9\n","test['timestamp'] = test['timestamp'].view('int64') * 1e9\n","\n","# train_autogluon.drop(columns=['item_id', 'original_index'], inplace=True)\n","# test.drop(columns=['item_id', 'original_index'], inplace=True)\n","\n","# train_autogluon = train_autogluon.fillna(0)\n","# test = test.fillna(0)\n","\n","# cat_mean_col = ['total_item_value', 'item_month_Weekday', 'item_corp_Weekday', 'item_location_Weekday', 'item_year_season']\n","# for cat_col in cat_mean_col:\n","#     mean_value = pd.pivot_table(train_autogluon, values = 'price(원/kg)', index = [f'{cat_col}'], aggfunc = np.mean).reset_index()\n","#     tqdm.pandas()\n","#     train_autogluon[f'{cat_col}_mean'] = train_autogluon.progress_apply(lambda x : mean_value.loc[(mean_value[f'{cat_col}'] == x[f'{cat_col}']),'price(원/kg)'].values[0], axis = 1)\n","#     tqdm.pandas()\n","#     test[f'{cat_col}_mean'] = test.progress_apply(lambda x : mean_value.loc[(mean_value[f'{cat_col}'] == x[f'{cat_col}']) ,'price(원/kg)'].values[0], axis = 1)\n","\n","#     std_value = pd.pivot_table(train_autogluon, values = 'price(원/kg)', index = [f'{cat_col}'], aggfunc = np.std).reset_index()\n","#     tqdm.pandas()\n","#     train_autogluon[f'{cat_col}_std'] = train_autogluon.progress_apply(lambda x : std_value.loc[(std_value[f'{cat_col}'] == x[f'{cat_col}']),'price(원/kg)'].values[0], axis = 1)\n","#     tqdm.pandas()\n","#     test[f'{cat_col}_std'] = test.progress_apply(lambda x : std_value.loc[(std_value[f'{cat_col}'] == x[f'{cat_col}']) ,'price(원/kg)'].values[0], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpPdgFGfkx5H"},"outputs":[],"source":["hyperparameters = {\n","    'GBM': [\n","        {'device': 'gpu', 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n","        {'device': 'gpu'},\n","        'GBMLarge'\n","    ],\n","    'CAT': {'task_type': 'GPU'},\n","    'XGB': {'tree_method': 'gpu_hist'}\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vW0-luRgvvt"},"outputs":[],"source":["label = 'price(원/kg)'\n","predictor = TabularPredictor(label=label, path=path + 'auto_result').fit(train_autogluon,\n","                            ag_args_fit={'num_gpus': 1},\n","                            hyperparameters=hyperparameters,\n","                            time_limit=3600, presets='best_quality')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYU7EGIfPOds"},"outputs":[],"source":["model_list = list(predictor.leaderborad()['model'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcJhHHd6PWzs"},"outputs":[],"source":["for model in model_list:\n","    predictions = predictor.predict(test, model=f'{model}')\n","    submission = pd.read_csv(path + 'sample_submission.csv')\n","\n","    submission[\"item_id\"] = submission[\"ID\"].str[0:6]\n","    submission[\"TIME\"] = submission[\"ID\"].str[10:]\n","\n","    submission = submission[(submission[\"item_id\"] != \"BC_B_S\") & (submission[\"item_id\"] != \"BC_C_S\") & (submission[\"item_id\"] != \"CB_A_S\") & (submission[\"item_id\"] != \"CR_D_S\") & (submission[\"item_id\"] != \"CR_E_S\") & (submission[\"item_id\"] != \"RD_C_S\")]\n","    submission = submission.reset_index(drop = True)\n","\n","    submission['answer'] = np.round(predictions)\n","    sunday_ids_march_2023 = [f\"_202303{day:02}\" for day in [5, 12, 19, 26]]\n","    submission.loc[submission['answer'] < 0.0, 'answer'] = 0.0\n","    for sunday_id in sunday_ids_march_2023:\n","        submission.loc[submission['ID'].str.contains(sunday_id), 'answer'] = 0\n","    # submission.to_csv(path + f'auto_mb_sep/{model}_go.csv', index=False)\n","\n","    sb = pd.read_csv(path + 'autogluon_submission_feature2.csv')\n","    sb[\"item_id\"] = sb[\"ID\"].str[0:6]\n","    sb[\"TIME\"] = sb[\"ID\"].str[10:]\n","\n","    sb['original_order'] = range(len(sb))\n","\n","    # 인덱스 설정\n","    submission.set_index(['item_id', 'TIME'], inplace=True)\n","    sb.set_index(['item_id', 'TIME'], inplace=True)\n","\n","    # 공통 인덱스 찾기\n","    common_indices = sb.index.intersection(submission.index)\n","\n","    # 'original_order' 열 보존하면서 sb 업데이트\n","    sb_update = sb.loc[common_indices].copy()\n","    sb_update.update(submission.loc[common_indices])\n","    sb.loc[common_indices] = sb_update\n","\n","    # 인덱스 리셋 및 원래 순서로 정렬\n","    sb.reset_index(inplace=True)\n","    sb.sort_values(by='original_order', inplace=True)\n","\n","    # 불필요한 'original_order' 열 제거\n","    sb.drop(columns=['original_order'], inplace=True)\n","    sb.drop(['item_id','TIME'],axis=1,inplace=True)\n","    sb.to_csv(path + f'autogluon_regression_final_{model}.csv', index=False)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
