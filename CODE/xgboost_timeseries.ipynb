{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2288,"status":"ok","timestamp":1700363755036,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"Xebjo9x2SlZ9","outputId":"f08309da-723b-4ac5-bdc6-c7edb23ea7c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"Xebjo9x2SlZ9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zuVnymPSmPj"},"outputs":[],"source":["!pip install sktime optuna"],"id":"_zuVnymPSmPj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"civilian-stations"},"outputs":[],"source":["import sys\n","import sktime\n","import tqdm as tq\n","import xgboost as xgb\n","import matplotlib\n","import seaborn as sns\n","import sklearn as skl\n","import pandas as pd\n","import numpy as np\n","import optuna\n","from sklearn.metrics import mean_squared_error"],"id":"civilian-stations"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6b5e25e-f9a1-4980-ac53-6cdb73b3d0d4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sktime.forecasting.model_selection import temporal_train_test_split\n","from sktime.utils.plotting import plot_series\n","from xgboost import XGBRegressor\n","from sklearn.preprocessing import LabelEncoder\n","import holidays\n","\n","pd.set_option('display.max_columns', 30)\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"id":"d6b5e25e-f9a1-4980-ac53-6cdb73b3d0d4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc71a8oFSo4B"},"outputs":[],"source":["path = '/content/drive/MyDrive/DACON/jeju_price/'"],"id":"Rc71a8oFSo4B"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b48de8a-aec9-42d7-8024-082a0c86e1a2"},"outputs":[],"source":["train = pd.read_csv(path + 'train.csv')\n","test = pd.read_csv(path + 'test.csv')\n","\n","## 변수들을 영문명으로 변경\n","cols = ['id', 'date_time', 'item', 'corporation', 'location', 'supply', 'price']\n","train.columns = cols\n","cols = ['id', 'date_time', 'item', 'corporation', 'location']\n","test.columns = cols\n","\n","\n","def group_season(df):\n","    df.loc[(df['month'] == 3) | (df['month'] == 4) | (df['month'] == 5), 'season'] = '봄'\n","    df.loc[(df['month'] == 6) | (df['month'] == 7) | (df['month'] == 8), 'season'] = '여름'\n","    df.loc[(df['month'] == 9) | (df['month'] == 10) | (df['month'] == 11), 'season'] = '가을'\n","    df.loc[(df['month'] == 12) | (df['month'] == 1) | (df['month'] == 2), 'season'] = '겨울'\n","    return df['season']\n","\n","def holiday(df):\n","    kr_holidays = holidays.KR()\n","    df['holiday'] = df.date_time.apply(lambda x: 'holiday' if x in kr_holidays else 'non-holiday')\n","    return df['holiday']\n","\n","def cyclical_feature(df, time=12):\n","    df['sin_time'] = np.sin(2*np.pi*df.month/time)\n","    df['cos_time'] = np.cos(2*np.pi*df.month/time)\n","\n","def post_preprocessing(test, submission):\n","    idx_list = test[(test['Weekday'] == 6)].index\n","    submission.loc[idx_list, 'answer'] = 0 # Weekday == 6 (일요일)이면 가격 0원\n","    submission['answer'] = submission['answer'].apply(lambda x: max(0, x)) # 가격에 음수가 있다면 가격 0원으로 변경\n","    return submission\n","\n","\n","\n","# 날짜를 기반으로 주 수확 시기인지를 판단하는 함수를 정의합니다.\n","def determine_harvest_weight(item, month):\n","    harvest_times = {\n","    'TG': {'main': [(10, 1)]},  # 감귤: 10월부터 이듬해 1월까지\n","    'BC': {'main': [(4, 6), (9, 11)]},  # 브로콜리: 4월-6월, 9월-11월\n","    'RD': {'main': [(5, 6), (11, 12)]},  # 무: 5월, 11월\n","    'CR': {'main': [(7, 8), (10, 11)]},  # 당근: 7월-8월, 10월-12월\n","    'CB': {'main': [(6, 6), (11, 11)]}  # 양배추: 6월, 11월\n","}\n","    main_harvest = harvest_times[item]['main']\n","    for start, end in main_harvest:\n","        if start <= month <= end:\n","            return 1\n","    return 0\n","\n","class DataPreprocessing:\n","    def __init__(self, train, test):\n","        self.train = train\n","        self.test = test\n","\n","    @staticmethod\n","    def label_encode(train, test):\n","        categorical_col = ['item', 'corporation', 'location', 'season', 'holiday',\n","                           'item_month_Weekday', 'item_corp_Weekday', 'item_location_Weekday', 'item_year_season', 'item_weight']\n","\n","        for i in categorical_col:\n","            le = LabelEncoder()\n","            train[i] = le.fit_transform(train[i])\n","            test[i] = le.transform(test[i])\n","\n","        return train, test\n","\n","    @staticmethod\n","    def remove_outliers(train):\n","        print('Remove outliers')\n","        train.loc[(train['Weekday'] == 6) & (train['price'] >= 0), 'price'] = 0\n","        return train\n","\n","    @staticmethod\n","    def preprocessing(data):\n","        print('Preprocessing Start')\n","        # time feature\n","        data['year'] = data['date_time'].apply(lambda x: int(x[0:4]))\n","        data['month'] = data['date_time'].apply(lambda x: int(x[5:7]))\n","        data['Weekday'] = pd.to_datetime(data['date_time']).dt.weekday\n","        data['is_weekend'] = data['Weekday'].apply(lambda x: 1 if x >= 6 else 0)\n","        data['year'] = data['year'] - 2019\n","        data['season'] = group_season(data)\n","        data['holiday'] = holiday(data)\n","        cyclical_feature(data)\n","\n","        # item feature\n","        data['total_item_value'] = data['item']+ '_' + data['corporation']+ '_' +data['location']\n","        data['item_month_Weekday'] = data['item'].astype(str) + \"_\" + data['month'].astype(str) + data['Weekday'].astype(str)\n","        data['item_corp_Weekday'] = data['item'].astype(str) + \"_\" + data['corporation'].astype(str) + data['Weekday'].astype(str)\n","        data['item_location_Weekday'] = data['item'].astype(str) + \"_\" + data['location'].astype(str) + data['Weekday'].astype(str)\n","        data['item_year_season'] = data['item'].astype(str) + \"_\" + data['year'].astype(str) + \"_\" + data['season'].astype(str)\n","\n","\n","        data['date_time'] = pd.to_datetime(data['date_time'])\n","        data['days_in_month'] = data['date_time'].dt.days_in_month\n","        data['is_month_start'] = data['date_time'].dt.is_month_start\n","        data['is_month_end'] = data['date_time'].dt.is_month_end\n","        data['is_quarter_start'] = data['date_time'].dt.is_quarter_start\n","        data['is_quarter_end'] = data['date_time'].dt.is_quarter_end\n","        data['is_year_start'] = data['date_time'].dt.is_year_start\n","        data['is_year_end'] = data['date_time'].dt.is_year_end\n","        data['is_leap_year'] = data['date_time'].dt.is_leap_year\n","        data['dayofyear'] = data['date_time'].dt.dayofyear\n","        data['harvest_weight'] = data.apply(lambda row: determine_harvest_weight(row['item'], row['date_time'].month), axis=1)\n","        # data['date_time'] = data['date_time'].view('int64') * 1e9\n","\n","        data['item_weight'] = data['item'].astype(str) + \"_\" + data['harvest_weight'].astype(str)\n","        return data\n","\n","    def fit(self):\n","        self.train = self.preprocessing(self.train)\n","        self.test = self.preprocessing(self.test)\n","\n","        self.train = self.remove_outliers(self.train)\n","\n","        # x_train = self.train.drop(columns=['ID', 'supply(kg)', 'price'])\n","        # y_train = self.train['price']\n","        # x_test = self.test.drop(columns=['ID'])\n","\n","        self.train, self.test = self.label_encode(self.train, self.test)\n","\n","        return train, test\n","\n","preprocessing = DataPreprocessing(train, test)\n","train, test = preprocessing.fit()\n","\n","cat_mean_col = ['total_item_value', 'item_month_Weekday', 'item_corp_Weekday', 'item_location_Weekday', 'item_year_season']\n","for cat_col in cat_mean_col:\n","    mean_value = pd.pivot_table(train, values = 'price', index = [f'{cat_col}'], aggfunc = np.mean).reset_index()\n","    tqdm.pandas()\n","    train[f'{cat_col}_mean'] = train.progress_apply(lambda x : mean_value.loc[(mean_value[f'{cat_col}'] == x[f'{cat_col}']),'price'].values[0], axis = 1)\n","    tqdm.pandas()\n","    test[f'{cat_col}_mean'] = test.progress_apply(lambda x : mean_value.loc[(mean_value[f'{cat_col}'] == x[f'{cat_col}']) ,'price'].values[0], axis = 1)\n","\n","    std_value = pd.pivot_table(train, values = 'price', index = [f'{cat_col}'], aggfunc = np.std).reset_index()\n","    tqdm.pandas()\n","    train[f'{cat_col}_std'] = train.progress_apply(lambda x : std_value.loc[(std_value[f'{cat_col}'] == x[f'{cat_col}']),'price'].values[0], axis = 1)\n","    tqdm.pandas()\n","    test[f'{cat_col}_std'] = test.progress_apply(lambda x : std_value.loc[(std_value[f'{cat_col}'] == x[f'{cat_col}']) ,'price'].values[0], axis = 1)\n","\n","train.head()"],"id":"6b48de8a-aec9-42d7-8024-082a0c86e1a2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNLQFXP-fOiI"},"outputs":[],"source":["train['week_of_year'] = train['date_time'].dt.isocalendar().week\n","test['week_of_year'] = test['date_time'].dt.isocalendar().week\n","\n","train['week_item'] = train['week_of_year'].astype(str) + '_' + train['item'].astype(str)\n","test['week_item'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str)\n","filtered_data = train[train['price'] > 0]\n","mean_encoded = filtered_data.groupby('week_item')['price'].mean().reset_index()\n","mean_encoded.rename(columns={'price': 'mean_encoded_price'}, inplace=True)\n","train = pd.merge(train, mean_encoded, on='week_item', how='left')\n","test = pd.merge(test, mean_encoded, on='week_item', how='left')\n","\n","train['week_item_location'] = train['week_of_year'].astype(str) + '_' + train['item'].astype(str) + '_' + train['location'].astype(str)\n","test['week_item_location'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['location'].astype(str)\n","filtered_data = train[train['price'] > 0]\n","mean_encoded_mwic = filtered_data.groupby('week_item_location')['price'].mean().reset_index()\n","mean_encoded_mwic.rename(columns={'price': 'mean_encoded_price_mwil'}, inplace=True)\n","train = pd.merge(train, mean_encoded_mwic, on='week_item_location', how='left')\n","test = pd.merge(test, mean_encoded_mwic, on='week_item_location', how='left')\n","\n","train['week_item_corporation'] = train['week_of_year'].astype(str) + '_' + train['item'].astype(str) + '_' + train['corporation'].astype(str)\n","test['week_item_corporation'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['corporation'].astype(str)\n","filtered_data_mwic = train[train['price'] > 0]\n","mean_encoded_mwic = filtered_data_mwic.groupby('week_item_corporation')['price'].mean().reset_index()\n","mean_encoded_mwic.rename(columns={'price': 'mean_encoded_price_mwic'}, inplace=True)\n","train = pd.merge(train, mean_encoded_mwic, on='week_item_corporation', how='left')\n","test = pd.merge(test, mean_encoded_mwic, on='week_item_corporation', how='left')\n","\n","train['week_item_corporation_location'] = train['week_of_year'].astype(str) + '_' + train['item'].astype(str) + '_' + train['corporation'].astype(str) + '_' + train['location'].astype(str)\n","test['week_item_corporation_location'] = test['week_of_year'].astype(str) + '_' + test['item'].astype(str) + '_' + test['corporation'].astype(str) + '_' + test['location'].astype(str)\n","filtered_data_mwicl = train[train['price'] > 0]\n","mean_encoded_mwicl = filtered_data_mwicl.groupby('week_item_corporation_location')['price'].mean().reset_index()\n","mean_encoded_mwicl.rename(columns={'price': 'mean_encoded_price_mwicl'}, inplace=True)\n","train = pd.merge(train, mean_encoded_mwicl, on='week_item_corporation_location', how='left')\n","test = pd.merge(test, mean_encoded_mwicl, on='week_item_corporation_location', how='left')\n","\n","\n","# train['original_index'] = train.index\n","# test['original_index'] = test.index\n","\n","# train['item_id'] = train['item'].astype(str) + '_' + train['corporation'].astype(str) + '_' + train['location'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['corporation'].astype(str) + '_' + test['location'].astype(str)\n","# weekly_avg = train.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwicl'].mean().reset_index()\n","# weekly_avg['weekly_difference'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwicl'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference']]\n","# train = pd.merge(train, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train = train.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","\n","# train['item_id'] = train['item'].astype(str) + '_' + train['corporation'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['corporation'].astype(str)\n","# weekly_avg = train.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwic'].mean().reset_index()\n","# weekly_avg['weekly_difference_2'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwic'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_2']]\n","# train = pd.merge(train, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train = train.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","# train['item_id'] = train['item'].astype(str) + '_' + train['location'].astype(str)\n","# test['item_id'] = test['item'].astype(str) + '_' + test['location'].astype(str)\n","# weekly_avg = train.groupby(['item_id', 'week_of_year'])['mean_encoded_price_mwil'].mean().reset_index()\n","# weekly_avg['weekly_difference_3'] = weekly_avg.groupby('item_id')['mean_encoded_price_mwil'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_3']]\n","# train = pd.merge(train, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train = train.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","# train['item_id'] = train['item'].astype(str)\n","# test['item_id'] = test['item'].astype(str)\n","# weekly_avg = train.groupby(['item_id', 'week_of_year'])['mean_encoded_price'].mean().reset_index()\n","# weekly_avg['weekly_difference_4'] = weekly_avg.groupby('item_id')['mean_encoded_price'].diff()\n","# weekly_avg = weekly_avg[['item_id', 'week_of_year', 'weekly_difference_4']]\n","# train = pd.merge(train, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# test = pd.merge(test, weekly_avg, on=['item_id', 'week_of_year'], how='outer')\n","# train = train.sort_values(by='original_index')\n","# test = test.sort_values(by='original_index')\n","\n","train['date_time_int'] = train['date_time'].view('int64') * 1e9\n","test['date_time_int'] = test['date_time'].view('int64') * 1e9\n","\n","# train.drop(columns=['item_id', 'original_index', 'week_item_location', 'week_item_corporation', 'week_item_corporation_location', 'week_item', 'week_of_year'], inplace=True)\n","# test.drop(columns=['item_id', 'original_index',  'week_item_location', 'week_item_corporation', 'week_item_corporation_location', 'week_item', 'week_of_year'], inplace=True)\n","\n","# train = train.fillna(0)\n","# test = test.fillna(0)\n","\n","\n","# categorical_col = ['week_of_year']\n","\n","# for i in categorical_col:\n","#     le = LabelEncoder()\n","#     train[i] = le.fit_transform(train[i])\n","#     test[i] = le.transform(test[i])"],"id":"yNLQFXP-fOiI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_KlXs7Yi8Q0"},"outputs":[],"source":["train.drop(columns=['id', 'supply'], inplace=True)\n","train = train[['total_item_value', 'date_time', 'price', 'item', 'corporation', 'location',\n","       'year', 'month', 'Weekday', 'is_weekend', 'season', 'holiday',\n","       'sin_time', 'cos_time', 'item_month_Weekday',\n","       'item_corp_Weekday', 'item_location_Weekday', 'item_year_season',\n","       'days_in_month', 'is_month_start', 'is_month_end', 'is_quarter_start',\n","       'is_quarter_end', 'is_year_start', 'is_year_end', 'is_leap_year',\n","       'dayofyear', 'harvest_weight', 'item_weight', 'total_item_value_mean',\n","       'total_item_value_std', 'item_month_Weekday_mean',\n","       'item_month_Weekday_std', 'item_corp_Weekday_mean',\n","       'item_corp_Weekday_std', 'item_location_Weekday_mean',\n","       'item_location_Weekday_std', 'item_year_season_mean',\n","       'item_year_season_std']]"],"id":"u_KlXs7Yi8Q0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncA0wP1ji-z3"},"outputs":[],"source":["test.drop(columns=['id'], inplace=True)\n","test = test[['total_item_value', 'date_time', 'item', 'corporation', 'location', 'year', 'month',\n","       'Weekday', 'is_weekend', 'season', 'holiday', 'sin_time', 'cos_time',\n","       'item_month_Weekday', 'item_corp_Weekday',\n","       'item_location_Weekday', 'item_year_season', 'days_in_month',\n","       'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end',\n","       'is_year_start', 'is_year_end', 'is_leap_year', 'dayofyear',\n","       'harvest_weight', 'item_weight', 'total_item_value_mean',\n","       'total_item_value_std', 'item_month_Weekday_mean',\n","       'item_month_Weekday_std', 'item_corp_Weekday_mean',\n","       'item_corp_Weekday_std', 'item_location_Weekday_mean',\n","       'item_location_Weekday_std', 'item_year_season_mean',\n","       'item_year_season_std']]"],"id":"ncA0wP1ji-z3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"proved-chain","scrolled":true},"outputs":[],"source":["preds = np.array([])\n","item_unique = list(train['total_item_value'].unique())\n","for i in tqdm(item_unique):\n","    pred_df = pd.DataFrame(columns=['answer'])\n","\n","    y_train = train.loc[train.total_item_value == i, 'price']\n","    x_train, x_test = train.loc[train.total_item_value == i, ].iloc[:, 3:], test.loc[test.total_item_value == i, ].iloc[:,1:]\n","    x_test = x_test[x_train.columns]\n","\n","    model = xgb.XGBRegressor(**params)\n","\n","    model.fit(x_train, y_train)\n","    y_pred = model.predict(x_test)\n","    pred_df.loc[:,'answer'] = y_pred\n","\n","    pred = pred_df['answer']\n","    preds = np.append(preds, pred)\n","\n","submission = pd.read_csv(path + 'sample_submission.csv')\n","submission['answer'] = preds\n","submission.to_csv(path + 'xgboost_timeseries.csv', index=False)"],"id":"proved-chain"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FCvutYZcXaK"},"outputs":[],"source":[],"id":"3FCvutYZcXaK"}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"[final]","language":"python","name":"power_final"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}